{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jH_uDPwi6eD",
        "outputId": "e088cd15-a253-44b3-bbce-081c88312836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 22500 samples\n",
            "split\n",
            "train    15000\n",
            "test      4500\n",
            "val       3000\n",
            "Name: count, dtype: int64\n",
            "      char_count                                                      \\\n",
            "           count       mean        std  min   25%   50%   75%    max   \n",
            "split                                                                  \n",
            "test      4500.0  39.308889  15.760182  2.0  28.0  37.0  48.0  125.0   \n",
            "train    15000.0  39.906067  15.262904  2.0  29.0  38.0  49.0  136.0   \n",
            "val       3000.0  39.825667  16.580860  2.0  28.0  37.0  49.0  114.0   \n",
            "\n",
            "      word_count                                                 \n",
            "           count      mean       std  min  25%  50%   75%   max  \n",
            "split                                                            \n",
            "test      4500.0  8.191111  3.242021  1.0  6.0  8.0  10.0  25.0  \n",
            "train    15000.0  8.339200  3.191868  1.0  6.0  8.0  10.0  28.0  \n",
            "val       3000.0  8.319333  3.434330  1.0  6.0  8.0  10.0  24.0  \n",
            "Top 5 intents by sample count (train):\n",
            "intent\n",
            "translate          100\n",
            "transfer           100\n",
            "timer              100\n",
            "definition         100\n",
            "meaning_of_life    100\n",
            "Name: count, dtype: int64\n",
            "Bottom 5 intents by sample count (train):\n",
            "intent\n",
            "income           100\n",
            "order            100\n",
            "traffic          100\n",
            "order_checks     100\n",
            "card_declined    100\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:00<00:00, 220720.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common tokens (train): [('my', 5528), ('i', 5437), ('to', 5428), ('the', 3992), ('you', 2966), ('a', 2531), ('for', 2520), ('what', 2506), ('me', 2420), ('is', 2235), ('how', 2065), ('do', 1912), ('on', 1797), ('can', 1793), ('in', 1647), ('of', 1466), ('need', 1306), ('please', 1134), ('card', 1103), ('tell', 998)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ---- Config ----\n",
        "DATA_FILE = \"/content/data_full.json\"\n",
        "OUTPUT_DIR = \"eda_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Load data ----\n",
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Expecting data keys like 'train', 'val', 'test'\n",
        "records = []\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    for rec in data[split_name]:\n",
        "        records.append({\n",
        "            'split': split_name,\n",
        "            'text': rec[0],\n",
        "            'intent': rec[1]\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(f\"Loaded {len(df)} samples\")\n",
        "print(df['split'].value_counts())\n",
        "\n",
        "# ---- Basic stats ----\n",
        "df['char_count'] = df['text'].apply(len)\n",
        "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(df.groupby('split')[['char_count','word_count']].describe())\n",
        "\n",
        "# Distribution of classes (intents)\n",
        "intent_counts = df[df['split']=='train']['intent'].value_counts()\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(intent_counts, bins=50)\n",
        "plt.title(\"Train-split: Samples per Intent Class\")\n",
        "plt.xlabel(\"Number of samples\")\n",
        "plt.ylabel(\"Count of intent classes\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"train_intent_counts_hist.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"Top 5 intents by sample count (train):\")\n",
        "print(intent_counts.head(5))\n",
        "print(\"Bottom 5 intents by sample count (train):\")\n",
        "print(intent_counts.tail(5))\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(df['word_count'], bins=100)\n",
        "plt.title(\"Distribution of Word Counts (All splits)\")\n",
        "plt.xlabel(\"Words per sample\")\n",
        "plt.ylabel(\"Count of samples\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"word_count_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "\n",
        "oos_label = 'oos'\n",
        "df['is_oos'] = (df['intent']==oos_label)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x='is_oos', y='word_count', data=df)\n",
        "plt.title(\"Word count: In-domain vs OOS\")\n",
        "plt.xlabel(\"Is OOS?\")\n",
        "plt.ylabel(\"Words per sample\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"word_count_oos_vs_indomain.png\"))\n",
        "plt.close()\n",
        "\n",
        "from tqdm import tqdm\n",
        "counter = Counter()\n",
        "for txt in tqdm(df[df['split']=='train']['text']):\n",
        "    tokens = txt.split()\n",
        "    counter.update(tokens)\n",
        "\n",
        "most_common = counter.most_common(50)\n",
        "print(\"Most common tokens (train):\", most_common[:20])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# ðŸ”¹ PREPROCESSING + ENHANCED EDA\n",
        "# =====================================\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# ---- Clean text function ----\n",
        "def clean_text(text):\n",
        "    text = text.lower()                              # lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)        # remove URLs\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)              # remove punctuation & numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()          # normalize spaces\n",
        "    text = \" \".join([w for w in text.split() if w not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print(\"âœ… Text cleaning done. Example:\")\n",
        "print(df[['text','clean_text']].head(3))\n",
        "\n",
        "# ---- Basic comparisons ----\n",
        "df['clean_word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "print(\"\\nWord count comparison (raw vs cleaned):\")\n",
        "print(df[['word_count','clean_word_count']].describe())\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.histplot(df['clean_word_count'], bins=100, color='teal', alpha=0.7)\n",
        "plt.title(\"Distribution of Cleaned Word Counts (All splits)\")\n",
        "plt.xlabel(\"Words per sample (cleaned)\")\n",
        "plt.ylabel(\"Count of samples\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"word_count_cleaned_distribution.png\"))\n",
        "plt.close()\n",
        "\n",
        "# ---- Word Cloud for overall corpus ----\n",
        "from wordcloud import STOPWORDS\n",
        "text_corpus = \" \".join(df[df['split']=='train']['clean_text'].values)\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color=\"white\",\n",
        "                      stopwords=STOPWORDS, max_words=200).generate(text_corpus)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud â€” All Training Texts (Cleaned)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"wordcloud_train_cleaned.png\"))\n",
        "plt.close()\n",
        "\n",
        "# ---- Optional: Word clouds for top few intents ----\n",
        "top_intents = df[df['split']=='train']['intent'].value_counts().head(3).index.tolist()\n",
        "for intent in top_intents:\n",
        "    text_joined = \" \".join(df[(df['split']=='train') & (df['intent']==intent)]['clean_text'].values)\n",
        "    wc = WordCloud(width=1200, height=600, background_color=\"white\").generate(text_joined)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud â€” Intent: {intent}\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, f\"wordcloud_{intent}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "print(f\"âœ… Word clouds generated for top intents in {OUTPUT_DIR}\")\n",
        "\n",
        "# ---- TF-IDF Analysis ----\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(df[df['split']=='train']['clean_text'])\n",
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "# Find top words per intent (based on mean TF-IDF)\n",
        "top_words_per_intent = {}\n",
        "train_df = df[df['split']=='train']\n",
        "for intent in train_df['intent'].unique():\n",
        "    subset = train_df[train_df['intent']==intent]\n",
        "    X_sub = tfidf.transform(subset['clean_text'])\n",
        "    mean_tfidf = np.asarray(X_sub.mean(axis=0)).ravel()\n",
        "    top_idx = mean_tfidf.argsort()[-10:][::-1]\n",
        "    top_words = feature_names[top_idx]\n",
        "    top_words_per_intent[intent] = top_words\n",
        "\n",
        "print(\"\\nTop words for a few sample intents:\")\n",
        "for k in list(top_words_per_intent.keys())[:5]:\n",
        "    print(f\"{k}: {', '.join(top_words_per_intent[k])}\")\n",
        "\n",
        "# ---- TF-IDF global importance plot ----\n",
        "mean_tfidf_global = np.asarray(X_tfidf.mean(axis=0)).ravel()\n",
        "top_global_idx = mean_tfidf_global.argsort()[-30:][::-1]\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=mean_tfidf_global[top_global_idx], y=feature_names[top_global_idx])\n",
        "plt.title(\"Top 30 Most Informative Words (TF-IDF Global, Train)\")\n",
        "plt.xlabel(\"Mean TF-IDF score\")\n",
        "plt.ylabel(\"Word\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"tfidf_top_words.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"âœ… TF-IDF analysis complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VrQbkjmnW_U",
        "outputId": "45912008-76fd-42fa-8c6d-979fd034432f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Text cleaning done. Example:\n",
            "                                                text  \\\n",
            "0  what expression would i use to say i love you ...   \n",
            "1  can you tell me how to say 'i do not speak muc...   \n",
            "2  what is the equivalent of, 'life is good' in f...   \n",
            "\n",
            "                              clean_text  \n",
            "0  expression would use say love italian  \n",
            "1    tell say speak much spanish spanish  \n",
            "2            equivalent life good french  \n",
            "\n",
            "Word count comparison (raw vs cleaned):\n",
            "         word_count  clean_word_count\n",
            "count  22500.000000      22500.000000\n",
            "mean       8.306933          4.052222\n",
            "std        3.235613          1.713561\n",
            "min        1.000000          0.000000\n",
            "25%        6.000000          3.000000\n",
            "50%        8.000000          4.000000\n",
            "75%       10.000000          5.000000\n",
            "max       28.000000         14.000000\n",
            "âœ… Word clouds generated for top intents in eda_outputs\n",
            "\n",
            "Top words for a few sample intents:\n",
            "translate: say, hello, french, spanish, translate, would, thank, english, love, goodbye\n",
            "transfer: transfer, account, checking, savings, money, dollars, send, one, move, another\n",
            "timer: timer, set, minutes, minute, please, two, start, need, seconds, want\n",
            "definition: definition, mean, meaning, means, word, define, flange, ataraxy, tell, zesty\n",
            "meaning_of_life: life, meaning, purpose, whats, lifes, point, think, tell, know, existence\n",
            "âœ… TF-IDF analysis complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all unique intent classes\n",
        "intent_classes = sorted(df['intent'].unique())\n",
        "print(f\"Total number of intent classes: {len(intent_classes)}\\n\")\n",
        "\n",
        "# Display them neatly\n",
        "for i, intent in enumerate(intent_classes, 1):\n",
        "    print(f\"{i:3d}. {intent}\")\n"
      ],
      "metadata": {
        "id": "Z3AZzxxAoTi0",
        "outputId": "5f2fd694-2628-4a70-c00a-404f73ef49cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of intent classes: 150\n",
            "\n",
            "  1. accept_reservations\n",
            "  2. account_blocked\n",
            "  3. alarm\n",
            "  4. application_status\n",
            "  5. apr\n",
            "  6. are_you_a_bot\n",
            "  7. balance\n",
            "  8. bill_balance\n",
            "  9. bill_due\n",
            " 10. book_flight\n",
            " 11. book_hotel\n",
            " 12. calculator\n",
            " 13. calendar\n",
            " 14. calendar_update\n",
            " 15. calories\n",
            " 16. cancel\n",
            " 17. cancel_reservation\n",
            " 18. car_rental\n",
            " 19. card_declined\n",
            " 20. carry_on\n",
            " 21. change_accent\n",
            " 22. change_ai_name\n",
            " 23. change_language\n",
            " 24. change_speed\n",
            " 25. change_user_name\n",
            " 26. change_volume\n",
            " 27. confirm_reservation\n",
            " 28. cook_time\n",
            " 29. credit_limit\n",
            " 30. credit_limit_change\n",
            " 31. credit_score\n",
            " 32. current_location\n",
            " 33. damaged_card\n",
            " 34. date\n",
            " 35. definition\n",
            " 36. direct_deposit\n",
            " 37. directions\n",
            " 38. distance\n",
            " 39. do_you_have_pets\n",
            " 40. exchange_rate\n",
            " 41. expiration_date\n",
            " 42. find_phone\n",
            " 43. flight_status\n",
            " 44. flip_coin\n",
            " 45. food_last\n",
            " 46. freeze_account\n",
            " 47. fun_fact\n",
            " 48. gas\n",
            " 49. gas_type\n",
            " 50. goodbye\n",
            " 51. greeting\n",
            " 52. how_busy\n",
            " 53. how_old_are_you\n",
            " 54. improve_credit_score\n",
            " 55. income\n",
            " 56. ingredient_substitution\n",
            " 57. ingredients_list\n",
            " 58. insurance\n",
            " 59. insurance_change\n",
            " 60. interest_rate\n",
            " 61. international_fees\n",
            " 62. international_visa\n",
            " 63. jump_start\n",
            " 64. last_maintenance\n",
            " 65. lost_luggage\n",
            " 66. make_call\n",
            " 67. maybe\n",
            " 68. meal_suggestion\n",
            " 69. meaning_of_life\n",
            " 70. measurement_conversion\n",
            " 71. meeting_schedule\n",
            " 72. min_payment\n",
            " 73. mpg\n",
            " 74. new_card\n",
            " 75. next_holiday\n",
            " 76. next_song\n",
            " 77. no\n",
            " 78. nutrition_info\n",
            " 79. oil_change_how\n",
            " 80. oil_change_when\n",
            " 81. order\n",
            " 82. order_checks\n",
            " 83. order_status\n",
            " 84. pay_bill\n",
            " 85. payday\n",
            " 86. pin_change\n",
            " 87. play_music\n",
            " 88. plug_type\n",
            " 89. pto_balance\n",
            " 90. pto_request\n",
            " 91. pto_request_status\n",
            " 92. pto_used\n",
            " 93. recipe\n",
            " 94. redeem_rewards\n",
            " 95. reminder\n",
            " 96. reminder_update\n",
            " 97. repeat\n",
            " 98. replacement_card_duration\n",
            " 99. report_fraud\n",
            "100. report_lost_card\n",
            "101. reset_settings\n",
            "102. restaurant_reservation\n",
            "103. restaurant_reviews\n",
            "104. restaurant_suggestion\n",
            "105. rewards_balance\n",
            "106. roll_dice\n",
            "107. rollover_401k\n",
            "108. routing\n",
            "109. schedule_maintenance\n",
            "110. schedule_meeting\n",
            "111. share_location\n",
            "112. shopping_list\n",
            "113. shopping_list_update\n",
            "114. smart_home\n",
            "115. spelling\n",
            "116. spending_history\n",
            "117. sync_device\n",
            "118. taxes\n",
            "119. tell_joke\n",
            "120. text\n",
            "121. thank_you\n",
            "122. time\n",
            "123. timer\n",
            "124. timezone\n",
            "125. tire_change\n",
            "126. tire_pressure\n",
            "127. todo_list\n",
            "128. todo_list_update\n",
            "129. traffic\n",
            "130. transactions\n",
            "131. transfer\n",
            "132. translate\n",
            "133. travel_alert\n",
            "134. travel_notification\n",
            "135. travel_suggestion\n",
            "136. uber\n",
            "137. update_playlist\n",
            "138. user_name\n",
            "139. vaccines\n",
            "140. w2\n",
            "141. weather\n",
            "142. what_are_your_hobbies\n",
            "143. what_can_i_ask_you\n",
            "144. what_is_your_name\n",
            "145. what_song\n",
            "146. where_are_you_from\n",
            "147. whisper_mode\n",
            "148. who_do_you_work_for\n",
            "149. who_made_you\n",
            "150. yes\n"
          ]
        }
      ]
    }
  ]
}